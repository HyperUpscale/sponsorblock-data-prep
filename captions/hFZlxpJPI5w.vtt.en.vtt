WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.640
Dear Fellow Scholars, this is Two Minute Papers
with Dr. Károly Zsolnai-Fehér.

00:00:04.640 --> 00:00:09.460
With the ascendancy of neural network-based
learning algorithms, we are now able to take

00:00:09.460 --> 00:00:15.370
on, and defeat problems that sounded completely
impossible just a few years ago.

00:00:15.370 --> 00:00:20.880
For instance, now, we can create deepfakes,
or, in other words, we can record a short

00:00:20.880 --> 00:00:26.280
video of ourselves, and transfer our gestures
to a target subject, and this particular technique

00:00:26.280 --> 00:00:32.020
is so advanced, that we don’t even need
a video of our target, just one still image.

00:00:32.020 --> 00:00:39.800
So, we can even use paintings, images of sculptures,
so yes, even the Mona Lisa works!

00:00:39.800 --> 00:00:43.340
However, don’t despair, it’s not all doom
and gloom.

00:00:43.340 --> 00:00:49.350
A paper by the name FaceForensics contains
a large dataset of original and manipulated

00:00:49.350 --> 00:00:50.360
video pairs.

00:00:50.360 --> 00:00:56.020
As this offered a ton of training data for
real and forged videos, it became possible

00:00:56.020 --> 00:00:59.520
to use these to train a deepfake detector.

00:00:59.520 --> 00:01:05.119
You can see it here in action as these green
to red colors showcase regions that the AI

00:01:05.119 --> 00:01:07.710
correctly thinks were tampered with.

00:01:07.710 --> 00:01:13.490
However, if we have access to a deepfake detector,
we can also use it to improve our deepfake

00:01:13.490 --> 00:01:15.209
creating algorithms.

00:01:15.209 --> 00:01:17.969
And with this, an arms race has begun.

00:01:17.969 --> 00:01:21.930
The paper we are looking at today showcases
this phenomenon.

00:01:21.930 --> 00:01:27.829
If you look here, you see this footage which
is very visibly fake, and the algorithm correctly

00:01:27.829 --> 00:01:29.270
concludes that.

00:01:29.270 --> 00:01:35.100
Now, if you look at this video, which, for
us, looks like if it were the same video,

00:01:35.100 --> 00:01:42.079
yet it suddenly became real, at least the
AI thinks that, of course, incorrectly.

00:01:42.079 --> 00:01:44.049
This is very confusing.

00:01:44.049 --> 00:01:46.219
So what really happened here?

00:01:46.219 --> 00:01:51.369
To understand what is going on here, we first
have to talk about ostriches.

00:01:51.369 --> 00:01:55.029
So what do ostriches have to do with this
insanity?

00:01:55.029 --> 00:01:56.659
Let me try to explain that.

00:01:56.659 --> 00:02:01.069
An adversarial attack on a neural network
can be performed as follows.

00:02:01.069 --> 00:02:05.799
We present such a classifier network with
an image of a bus, and it will successfully

00:02:05.799 --> 00:02:09.110
tell us that yes, this is indeed a bus.

00:02:09.110 --> 00:02:10.890
Nothing too crazy here.

00:02:10.890 --> 00:02:17.099
Now, we show it not an image of a bus, but
a bus plus some carefully crafted noise that

00:02:17.099 --> 00:02:22.379
is barely perceptible, that forces the neural
network to misclassify it as an ostrich.

00:02:22.379 --> 00:02:27.250
I will stress that this is not any kind of
noise, but the kind of noise that exploits

00:02:27.250 --> 00:02:32.290
biases in the neural network, which is, by
no means trivial to craft.

00:02:32.290 --> 00:02:37.109
However, if we succeed at that, this kind
of adversarial attack can be pulled off on

00:02:37.109 --> 00:02:39.400
many different kinds of images.

00:02:39.400 --> 00:02:43.879
Everything that you see here on the right
will be classified as an ostrich by the neural

00:02:43.879 --> 00:02:46.560
network these noise patterns were crafted
for.

00:02:46.560 --> 00:02:52.409
And, this can now be done not only on images,
but videos as well, hence, what happened a

00:02:52.409 --> 00:02:57.890
minute ago is that the deepfake video has
been adversarially modified with noise to

00:02:57.890 --> 00:03:00.790
bypass such a detector.

00:03:00.790 --> 00:03:05.599
If you look here, you see that the authors
have chosen excellent examples, because some

00:03:05.599 --> 00:03:11.290
of these are clearly forged videos, which
is initially recognized by the detector algorithm,

00:03:11.290 --> 00:03:15.879
but after adding the adversarial noise to
it, the detector fails spectacularly.

00:03:15.879 --> 00:03:20.659
To demonstrate the utility of their technique,
they have chosen the other examples to be

00:03:20.659 --> 00:03:22.079
much more subtle.

00:03:22.079 --> 00:03:24.810
Now, let’s talk about one more question.

00:03:24.810 --> 00:03:27.469
We are talking about a detector algorithm.

00:03:27.469 --> 00:03:32.730
But there is not one detector out there, there
are many, and we can change the wiring of

00:03:32.730 --> 00:03:36.019
these neural networks to have even more variation.

00:03:36.019 --> 00:03:39.620
So what does it mean to fool a detector?

00:03:39.620 --> 00:03:40.620
Excellent question.

00:03:40.620 --> 00:03:45.530
The success rate of these adversarial videos
indeed depends on the deepfake detector we

00:03:45.530 --> 00:03:51.000
are up against, but, hold on to your papers,
because this success rate on uncompressed

00:03:51.000 --> 00:03:58.159
videos is over 98%, which is amazing, but,
note that when using video compression, this

00:03:58.159 --> 00:04:03.750
success rate may drop to 58 to 92% depending
on the detector.

00:04:03.750 --> 00:04:09.290
This means that video compression and some
other tricks involving image transformations

00:04:09.290 --> 00:04:13.249
still help us in defending against these adversarial
attacks.

00:04:13.249 --> 00:04:18.489
What I also really like about the paper is
that it discusses white and black-box attacks

00:04:18.489 --> 00:04:19.989
separately.

00:04:19.989 --> 00:04:24.539
In the white-box case, we know everything
about the inner workings of the detector,

00:04:24.539 --> 00:04:29.990
including the neural network architecture
and parameters, this is typically the easier

00:04:29.990 --> 00:04:30.990
case.

00:04:30.990 --> 00:04:35.970
But, the technique also does really well in
the black-box case, where we are not allowed

00:04:35.970 --> 00:04:41.860
to look under the hood of the detector, but
we can show it a few videos and see how it

00:04:41.860 --> 00:04:43.210
reacts to them.

00:04:43.210 --> 00:04:47.550
This is a really cool work that gives us a
more nuanced view about the current state

00:04:47.550 --> 00:04:50.849
of the art around deepfakes and deepfake detectors.

00:04:50.849 --> 00:04:55.400
I think it is best if we all know about the
fact that these tools exist.

00:04:55.400 --> 00:05:00.370
If you wish to help us with this endeavor,
please make sure to share this with your friends.

00:05:00.370 --> 00:05:01.370
Thank you.

00:05:01.370 --> 00:05:03.760
This episode has been supported by Lambda.

00:05:03.760 --> 00:05:08.790
If you're a researcher or a startup looking
for cheap GPU compute to run these algorithms,

00:05:08.790 --> 00:05:11.310
check out Lambda GPU Cloud.

00:05:11.310 --> 00:05:16.120
I've talked about Lambda's GPU workstations
in other videos and am happy to tell you that

00:05:16.120 --> 00:05:19.180
they're offering GPU cloud services as well.

00:05:19.180 --> 00:05:26.900
The Lambda GPU Cloud can train Imagenet to
93% accuracy for less than $19! Lambda's web-based

00:05:26.900 --> 00:05:31.289
IDE lets you easily access your instance right
in your browser.

00:05:31.289 --> 00:05:36.740
And finally, hold on to your papers, because
the Lambda GPU Cloud costs less than half

00:05:36.740 --> 00:05:39.060
of AWS and Azure.

00:05:39.060 --> 00:05:45.090
Make sure to go to lambdalabs.com/papers and
sign up for one of their amazing GPU instances

00:05:45.090 --> 00:05:46.090
today.

00:05:46.090 --> 00:05:49.460
Our thanks to Lambda for helping us make better
videos for you.

00:05:49.460 --> 00:05:52.490
Thanks for watching and for your generous
support, and I'll see you next time!

