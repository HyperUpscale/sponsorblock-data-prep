WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:05.029
Dear Fellow Scholars, this is Two Minute Papers
with Dr. Károly Zsolnai-Fehér.

00:00:05.029 --> 00:00:09.730
About two years ago, we worked on a neural
rendering system, which would perform light

00:00:09.730 --> 00:00:15.489
transport on this scene and guess how it would
change if we would change the material properties

00:00:15.489 --> 00:00:17.100
of this test object.

00:00:17.100 --> 00:00:22.190
It was able to closely match the output of
a real light simulation program, and, it was

00:00:22.190 --> 00:00:28.760
near instantaneous as it took less than 5
milliseconds instead of the 40-60 seconds

00:00:28.760 --> 00:00:31.930
the light transport algorithm usually requires.

00:00:31.930 --> 00:00:37.179
This technique went by the name Gaussian Material
Synthesis, and the learned quantities were

00:00:37.179 --> 00:00:38.519
material properties.

00:00:38.519 --> 00:00:45.739
But this new paper sets out to learn something
more difficult, and also, more general.

00:00:45.739 --> 00:00:50.390
We are talking about a 5D neural radiance
field representation.

00:00:50.390 --> 00:00:52.929
So what does this mean exactly?

00:00:52.929 --> 00:00:58.719
What this means is that we have 3 dimensions
for location and two for view direction, or

00:00:58.719 --> 00:01:04.360
in short, the input is where we are in space
and what are we looking at, and the resulting

00:01:04.360 --> 00:01:06.210
image of this view.

00:01:06.210 --> 00:01:12.970
So here, we take a bunch of this input data,
learn it, and synthesize new, previously unseen

00:01:12.970 --> 00:01:19.050
views of not just the materials in the scene,
but the entire scene itself.

00:01:19.050 --> 00:01:24.730
And here, we are talking not only digital
environments, but also, real scenes as well!

00:01:24.730 --> 00:01:30.410
Now that’s quite a value proposition, let’s
see if it can live up to this promise!

00:01:30.410 --> 00:01:31.680
Wow!

00:01:31.680 --> 00:01:32.680
So good.

00:01:32.680 --> 00:01:33.730
Love it!

00:01:33.730 --> 00:01:36.810
But, what is it really that we should be looking
at?

00:01:36.810 --> 00:01:38.890
What makes a good output here?

00:01:38.890 --> 00:01:44.450
The most challenging part is writing an algorithm
that is able to reproduce delicate, high-frequency

00:01:44.450 --> 00:01:47.710
details while having temporal coherence.

00:01:47.710 --> 00:01:49.600
So what does that mean?

00:01:49.600 --> 00:01:54.720
Well, in simpler words, we are looking for
sharp and smooth image sequences.

00:01:54.720 --> 00:01:59.880
Perfectly matte objects are easier to learn
here because they look the same from all directions,

00:01:59.880 --> 00:02:05.040
while glossier, more reflective materials
are significantly more difficult, because

00:02:05.040 --> 00:02:10.950
they change a great deal as we move our head
around, and this highly variant information

00:02:10.950 --> 00:02:14.170
is typically not present in the learned input
images.

00:02:14.170 --> 00:02:18.760
If you read the paper, you’ll see these
referred to as non-Lambertian materials.

00:02:18.760 --> 00:02:23.260
The paper and the video contains a ton of
examples of these view-dependent effects to

00:02:23.260 --> 00:02:28.670
demonstrate that these difficult scenes are
handled really well by this technique.

00:02:28.670 --> 00:02:30.670
Refractions also look great.

00:02:30.670 --> 00:02:36.540
Now, if we define difficulty as things that
change a lot when we change our position or

00:02:36.540 --> 00:02:41.940
view direction a little, not only the non-Lambertian
materials are going to give us headaches,

00:02:41.940 --> 00:02:45.020
occlusion can be challenging as well.

00:02:45.020 --> 00:02:49.770
For instance, you can see here how well it
handles the complex occlusion situation between

00:02:49.770 --> 00:02:55.710
the ribs of the skeleton here.

00:02:55.710 --> 00:03:00.840
It also has an understanding of depth, and
this depth information is so accurate, that

00:03:00.840 --> 00:03:06.720
we can do these nice augmented reality applications
where we put a new, virtual object in the

00:03:06.720 --> 00:03:12.959
scene and it correctly determines whether
it is in front of, or behind the real objects

00:03:12.959 --> 00:03:14.220
in the scene.

00:03:14.220 --> 00:03:21.710
Kind of what these new iPads do with their
LiDAR sensors, but without the sensor.

00:03:21.710 --> 00:03:25.160
As you see, this technique smokes the competition.

00:03:25.160 --> 00:03:30.450
So what do you know, entire real-world scenes
can be reproduced from only a few views by

00:03:30.450 --> 00:03:32.090
using neural networks.

00:03:32.090 --> 00:03:34.790
And the results are just out of this world.

00:03:34.790 --> 00:03:36.709
Absolutely amazing.

00:03:36.709 --> 00:03:41.240
What you see here is an instrumentation of
this exact paper we have talked about, which

00:03:41.240 --> 00:03:43.230
was made by Weights and Biases.

00:03:43.230 --> 00:03:48.910
I think organizing these experiments really
showcases the usability of their system.

00:03:48.910 --> 00:03:53.989
Weights &amp; Biases provides tools to track your
experiments in your deep learning projects.

00:03:53.989 --> 00:03:59.080
Their system is designed to save you a ton
of time and money, and it is actively used

00:03:59.080 --> 00:04:05.280
in projects at prestigious labs, such as OpenAI,
Toyota Research, GitHub, and more.

00:04:05.280 --> 00:04:11.120
And, the best part is that if you are an academic
or have an open source project, you can use

00:04:11.120 --> 00:04:12.630
their tools for free.

00:04:12.630 --> 00:04:15.440
It really is as good as it gets.

00:04:15.440 --> 00:04:21.590
Make sure to visit them through wandb.com/papers
or just click the link in the video description

00:04:21.590 --> 00:04:24.050
and you can get a free demo today.

00:04:24.050 --> 00:04:29.310
Our thanks to Weights &amp; Biases for their long-standing
support and for helping us make better videos

00:04:29.310 --> 00:04:30.310
for you.

00:04:30.310 --> 00:04:33.800
Thanks for watching and for your generous
support, and I'll see you next time!

