WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:03.700
Dear Fellow Scholars, this is Two Minute Papers
with Károly Zsolnai-Fehér.

00:00:03.700 --> 00:00:09.590
In this series, we often discuss a class of
techniques by the name image inpainting.

00:00:09.590 --> 00:00:15.210
Image inpainting methods are capable of filling
in missing details from a mostly intact image.

00:00:15.210 --> 00:00:19.850
You see the legendary PatchMatch algorithm
at work here, which is more than 10 years

00:00:19.850 --> 00:00:24.850
old, and it is a good old computer graphics
method with no machine learning in sight,

00:00:24.850 --> 00:00:30.980
and after so much time, 10 years is an eternity
in research years, it still punches way above

00:00:30.980 --> 00:00:31.980
its weight.

00:00:31.980 --> 00:00:37.290
However, with the ascendancy of neural network-based
learning methods, I am often wondering whether

00:00:37.290 --> 00:00:42.800
it would be possible to take a more difficult
problem, for instance, inpainting not just

00:00:42.800 --> 00:00:45.100
still images, but movies as well.

00:00:45.100 --> 00:00:50.239
For instance, let’s take and old old black
and white movie that suffers from missing

00:00:50.239 --> 00:00:57.040
data, flickering, blurriness, and interestingly,
even the contrast of the footage has changed

00:00:57.040 --> 00:00:58.680
as it faded over time.

00:00:58.680 --> 00:01:04.110
Well, hold on to your papers, because this
learning-based approach fixes all of these,

00:01:04.110 --> 00:01:05.530
and even more!

00:01:05.530 --> 00:01:10.350
Step number one is restoration, which takes
care of all of these artifacts and contrast

00:01:10.350 --> 00:01:11.350
issues.

00:01:11.350 --> 00:01:16.350
You can not only see how much better the restored
version is, but it is also reported what the

00:01:16.350 --> 00:01:18.280
technique did exactly.

00:01:18.280 --> 00:01:21.110
However, it does more.

00:01:21.110 --> 00:01:23.680
What more could we possibly ask for?

00:01:23.680 --> 00:01:25.550
Well, colorization!

00:01:25.550 --> 00:01:30.619
What it does is that it looks at only 6 colorized
reference images that we have to provide,

00:01:30.619 --> 00:01:35.439
and uses this as art direction and propagate
it to the remainder of the frames.

00:01:35.439 --> 00:01:38.600
And it does an absolutely amazing work at
that.

00:01:38.600 --> 00:01:42.970
It even tells us which reference image it
is looking at when colorizing some of these

00:01:42.970 --> 00:01:49.460
frames, so if something does not come out
favorably, we know which image to recolor.

00:01:49.460 --> 00:01:53.790
The architecture of the neural network that
is used for all this also has to follow the

00:01:53.790 --> 00:01:55.250
requirements appropriately.

00:01:55.250 --> 00:02:00.610
For instance, beyond the standard spatial
convolution layers, it also makes ample use

00:02:00.610 --> 00:02:05.159
of these blue temporal convolution layers,
which helps “smearing out” the colorization

00:02:05.159 --> 00:02:09.340
information from one reference image to multiple
frames.

00:02:09.340 --> 00:02:14.760
However, in research, a technique is rarely
the very first at doing something, and sure

00:02:14.760 --> 00:02:20.489
enough, this is not the first technique that
does this kind of restoration and colorization.

00:02:20.489 --> 00:02:23.900
So how does it compare to previously published
methods?

00:02:23.900 --> 00:02:26.010
Well, quite favorably.

00:02:26.010 --> 00:02:31.299
With previous methods, in some cases, the
colorization just appears and disappears over

00:02:31.299 --> 00:02:34.379
time, while it is much more stable here.

00:02:34.379 --> 00:02:39.480
Also, fewer artifacts make it to the final
footage, and since cleaning these up is one

00:02:39.480 --> 00:02:43.489
of the main objectives of these methods, that’s
also great news.

00:02:43.489 --> 00:02:48.199
If we look at some quantitative results, or
in other words, numbers that describe the

00:02:48.199 --> 00:02:55.219
difference, you see here that we get a 3-4
decibels cleaner image, which is outstanding.

00:02:55.219 --> 00:03:00.430
Note that the decibel scale is not linear,
but a logarithmic scale, therefore if you

00:03:00.430 --> 00:03:06.659
read 28 instead of 24, it does not mean that
it’s just approximately 15% better.

00:03:06.659 --> 00:03:09.519
It is a much, much more pronounced difference
than that.

00:03:09.519 --> 00:03:14.499
I think these results are approaching a state
where they are becoming close to good enough

00:03:14.499 --> 00:03:20.989
so that we can revive some of these old masterpiece
movies and give them a much-deserved facelift.

00:03:20.989 --> 00:03:22.650
What a time to be alive!

00:03:22.650 --> 00:03:25.720
This episode has been supported by Weights
&amp; Biases.

00:03:25.720 --> 00:03:29.939
Weights &amp; Biases provides tools to track your
experiments in your deep learning projects.

00:03:29.939 --> 00:03:35.029
It can save you a ton of time and money in
these projects and is being used by OpenAI,

00:03:35.029 --> 00:03:38.019
Toyota Research, Stanford and Berkeley.

00:03:38.019 --> 00:03:42.999
They also wrote a guide on the fundamentals
of neural networks where they explain in simple

00:03:42.999 --> 00:03:48.829
terms how to train a neural network properly,
what are the most common errors you can make,

00:03:48.829 --> 00:03:50.400
and how to fix them.

00:03:50.400 --> 00:03:53.260
It is really great, you got to have a look.

00:03:53.260 --> 00:03:59.510
So make sure to visit them through wandb.com/papers
or just click the link in the video description

00:03:59.510 --> 00:04:01.829
and you can get a free demo today.

00:04:01.829 --> 00:04:05.839
Our thanks to Weights &amp; Biases for helping
us make better videos for you.

00:04:05.839 --> 00:04:08.799
Thanks for watching and for your generous
support, and I'll see you next time!

